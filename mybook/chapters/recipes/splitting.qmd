# 레시피: 대용량 파일을 쪼개서 병렬 처리하기 (Splitting)

매우 큰 데이터 파일을 처리할 때, 파일 전체를 하나의 프로세스로 처리하는 것보다 여러 조각으로 나누어 병렬로 처리하는 것이 훨씬 빠를 때가 있습니다.

## 기본 전략

1.  대용량 입력 파일을 작은 조각(chunk)들로 나눕니다.
2.  나뉜 조각들에 대해 병렬로 작업을 수행합니다.
3.  (필요한 경우) 개별 결과를 다시 하나로 합칩니다.

## Snakemake 예시

유닉스의 `split` 명령어를 사용하여 대용량 텍스트 파일을 조각내는 워크플로입니다.

```python
rule all:
    input:
        "results/processed_combined.txt"

# 1. 파일 쪼개기
rule split_file:
    input:
        "data/large_data.txt"
    output:
        temp(expand("data/chunks/part_{num}.txt", num=["00", "01", "02", "03"]))
    shell:
        # 1000줄씩 나누어 'part_' 접두사를 붙여 저장
        "split -l 1000 {input} data/chunks/part_"

# 2. 각 조각 처리 (와일드카드 활용)
rule process_chunk:
    input:
        "data/chunks/part_{num}.txt"
    output:
        "data/chunks/processed_{num}.txt"
    shell:
        "some_analysis_tool {input} > {output}"

# 3. 결과 합치기
rule combine_results:
    input:
        expand("data/chunks/processed_{num}.txt", num=["00", "01", "02", "03"])
    output:
        "results/processed_combined.txt"
    shell:
        "cat {input} > {output}"
```

## 참고 사항

*   **`temp()` 함수**: 쪼개진 조각들은 최종 결과가 만들어진 후에는 필요 없는 경우가 많습니다. `temp()`를 사용하면 워크플로가 끝나고 자동으로 삭제됩니다.
*   **Checkpoint**: 만약 파일을 몇 조각으로 나눌지 미리 알 수 없는 경우(예: 파일 크기에 따라 유동적인 경우), Snakemake의 `checkpoint` 기능을 사용하여 동적인 병렬 처리를 구현할 수 있습니다.
*   **도구 고유 기능**: `split` 명령어 외에도 생물정보학 도구(예: `fastp`, `samtools`) 중에는 자체적으로 데이터를 쪼개어 출력하는 기능을 가진 것들이 많으니 먼저 확인해 보세요.
